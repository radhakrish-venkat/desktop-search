# Desktop Search

A fast and efficient local document search tool that indexes and searches through various file formats including PDF, DOCX, XLSX, PPTX, and TXT files. Built with Python and featuring a clean command-line interface.

## Features

### **Core Search Capabilities**
- **Multi-format Support**: Indexes PDF, DOCX, XLSX, PPTX, and TXT files
- **Fast Keyword Search**: Uses inverted indexing for quick search results
- **Smart Ranking**: TF-IDF scoring for relevant result ranking
- **Snippet Generation**: Shows context around search terms with highlighting
- **Persistent Indexing**: Save and load indexes for repeated use

### **Google Drive Integration**
- **Cloud File Support**: Index and search Google Drive files including Google Docs, Sheets, and Slides
- **Hybrid Indexing**: Combine local files and Google Drive files in a single searchable index
- **Offline Search**: Once indexed, search Google Drive content without internet connection
- **Real-time Search**: Direct Google Drive API search without pre-building indexes
- **Secure Authentication**: OAuth2 with read-only access to your Google Drive

### **Advanced Semantic Search**
- **Semantic Understanding**: Find related content even without exact keyword matches
- **ChromaDB Integration**: Vector database for storing document embeddings
- **Sentence Transformers**: State-of-the-art embedding models for semantic similarity
- **Hybrid Semantic Search**: Combines semantic and keyword matching for optimal results
- **Configurable Models**: Support for different sentence transformer models
- **Similarity Thresholds**: Adjustable similarity scores for result filtering

### **Unified Search Experience**
- **Cross-Platform Search**: Search across local files and Google Drive simultaneously
- **Source Transparency**: Clear indication of result sources (local vs cloud)
- **Unified Results**: Consistent ranking and display across all file sources
- **Smart Indexing**: Automatically detects existing indexes and uses incremental updates
- **Incremental Indexing**: Add new files without rebuilding entire index
- **Smart Change Detection**: Tracks file modifications using metadata and hashes

### **Developer Experience**
- **Type Safety**: Full type hints throughout the codebase
- **Comprehensive Testing**: Unit and integration tests for all components
- **Robust Error Handling**: Graceful handling of corrupted or unsupported files
- **Progress Tracking**: Shows indexing progress for large directories
- **Modular Architecture**: Clean separation of concerns with extensible design

## Advantages

### **üöÄ Performance Benefits**
- **Lightning Fast Search**: Inverted indexing provides sub-second search results
- **Offline Capability**: Search Google Drive content without internet connection after indexing
- **Efficient Memory Usage**: Smart file size limits and chunking prevent memory issues
- **Scalable Architecture**: Handles large document collections with progress tracking

### **üîç Search Quality**
- **Semantic Intelligence**: Understands context and meaning, not just keywords
- **Hybrid Matching**: Combines semantic similarity with traditional keyword search
- **Smart Ranking**: TF-IDF scoring ensures most relevant results appear first
- **Context-Aware Snippets**: Shows relevant text portions with search term highlighting

### **‚òÅÔ∏è Cloud Integration**
- **Seamless Hybrid Search**: Search local and cloud files with a single command
- **Secure Cloud Access**: OAuth2 authentication with minimal permissions (read-only)
- **Real-time Updates**: Direct Google Drive API search for fresh results
- **Offline Independence**: Once indexed, no internet required for searching

### **üõ†Ô∏è Developer Friendly**
- **Type Safety**: Full type annotations for better code quality and IDE support
- **Comprehensive Testing**: High test coverage with unit and integration tests
- **Modular Design**: Clean architecture makes it easy to extend and maintain
- **Rich CLI**: Intuitive command-line interface with helpful error messages

### **üîí Privacy & Security**
- **Local Processing**: All search processing happens on your machine
- **No Data Mining**: Your documents and search queries never leave your system
- **Minimal Permissions**: Google Drive integration only requests read access
- **Secure Storage**: Credentials and tokens stored locally with proper encryption

### **üìä Enterprise Ready**
- **Multiple File Formats**: Support for all major document types
- **Configurable Models**: Choose the best embedding model for your use case
- **Smart Indexing**: Automatically chooses incremental or full indexing based on existing state
- **Incremental Updates**: Add new files without rebuilding entire indexes
- **Smart Change Detection**: Tracks file modifications using metadata and hashes
- **Comprehensive Logging**: Detailed logging for debugging and monitoring

## Installation

1. Clone the repository:
```bash
git clone <repository-url>
cd desktop-search
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

### Dependencies

The following packages are required:
- `click` - Command-line interface
- `pypdf` - PDF text extraction
- `python-docx` - Word document parsing
- `openpyxl` - Excel spreadsheet parsing
- `python-pptx` - PowerPoint presentation parsing
- `chromadb` - Vector database for semantic search
- `sentence-transformers` - Sentence embeddings for semantic search
- `google-auth` - Google authentication
- `google-auth-oauthlib` - OAuth2 authentication for Google APIs
- `google-auth-httplib2` - HTTP client for Google APIs
- `google-api-python-client` - Google Drive API client

## Usage

### Basic Commands

**Index a directory (smart indexing):**
```bash
python main.py index /path/to/your/documents
```
This automatically detects if an index already exists and uses incremental indexing for faster updates.
Creates an `index.pkl` file in the indexed directory.

**Search for content:**
```bash
python main.py search "your search query"
```

### Advanced Usage

**Index and save to custom location:**
```bash
python main.py index /path/to/documents --save my_index.pkl
```

**Force full indexing (rebuild everything):**
```bash
python main.py index /path/to/documents --force-full
```

**Search with saved index:**
```bash
python main.py search "query" --load my_index.pkl
```

**Search in a specific directory (uses default index.pkl):**
```bash
python main.py search "query" --directory /path/to/documents
```

**Limit search results:**
```bash
python main.py search "query" --limit 5
```

**Index without saving to file:**
```bash
python main.py index /path/to/documents --no-save
```

**Get help:**
```bash
python main.py --help
python main.py index --help
python main.py search --help
```

### Semantic Search Commands

**Build semantic index (smart indexing):**
```bash
python main.py semantic-index /path/to/your/documents
```

**Force full semantic indexing:**
```bash
python main.py semantic-index /path/to/your/documents --force-full
```

**Semantic search:**
```bash
python main.py semantic-search "your semantic query"
```

**Hybrid search (semantic + keyword):**
```bash
python main.py semantic-search "your query" --hybrid
```

**Get semantic index statistics:**
```bash
python main.py semantic-stats
```

**Custom ChromaDB path:**
```bash
python main.py semantic-index /path/to/documents --db-path ./my_chroma_db
```

**Use different sentence transformer model:**
```bash
python main.py semantic-index /path/to/documents --model all-mpnet-base-v2
```

### Google Drive Integration

**Setup Google Drive credentials:**
```bash
python main.py setup-gdrive /path/to/credentials.json
```

**Index Google Drive files:**
```bash
# Index all files in root folder
python main.py gdrive-index

# Index specific folder
python main.py gdrive-index --folder-id "1ABC123DEF456"

# Index only PDF files
python main.py gdrive-index --query "mimeType contains 'pdf'"

# Index files modified in last 30 days
python main.py gdrive-index --query "modifiedTime > '2024-01-01T00:00:00'"
```

**Search Google Drive directly:**
```bash
# Search all Google Drive files
python main.py gdrive-search "your query"

# Search in specific folder
python main.py gdrive-search "your query" --folder-id "1ABC123DEF456"

# Limit results
python main.py gdrive-search "your query" --limit 5
```

**Hybrid indexing (local + Google Drive):**
```bash
# Index both local files and Google Drive files
python main.py hybrid-index /path/to/local/documents --gdrive-folder-id "1ABC123DEF456"

# With additional Google Drive query filter
python main.py hybrid-index /path/to/local/documents --gdrive-folder-id "1ABC123DEF456" --gdrive-query "mimeType contains 'document'"
```

**Hybrid semantic indexing (local + Google Drive with ChromaDB):**
```bash
# Build semantic index for both local and Google Drive files
python main.py hybrid-semantic-index /path/to/local/documents --gdrive-folder-id "1ABC123DEF456"

# With custom model and database path
python main.py hybrid-semantic-index /path/to/local/documents \
  --gdrive-folder-id "1ABC123DEF456" \
  --model all-mpnet-base-v2 \
  --db-path ./hybrid_chroma_db
```

**Hybrid semantic search:**
```bash
# Semantic search across local and Google Drive files
python main.py hybrid-semantic-search "machine learning algorithms"

# Hybrid search (semantic + keyword)
python main.py hybrid-semantic-search "data analysis techniques" --hybrid

# With custom threshold and limit
python main.py hybrid-semantic-search "project proposal" --threshold 0.5 --limit 20
```

## Google Drive Setup

### 1. Create Google Cloud Project

1. Go to the [Google Cloud Console](https://console.cloud.google.com/)
2. Create a new project or select an existing one
3. Enable the Google Drive API for your project

### 2. Create OAuth 2.0 Credentials

1. In the Google Cloud Console, go to "APIs & Services" > "Credentials"
2. Click "Create Credentials" > "OAuth 2.0 Client IDs"
3. Choose "Desktop application" as the application type
4. Download the credentials JSON file

### 3. Setup in Desktop Search

```bash
# Copy your credentials file to the application
python main.py setup-gdrive /path/to/downloaded/credentials.json
```

This will:
- Copy the credentials to `~/.config/desktop-search/credentials.json`
- Create the necessary directory structure
- Prepare the application for Google Drive authentication

### 4. First Authentication

When you first run a Google Drive command, the application will:
1. Open a browser window for OAuth authentication
2. Ask you to sign in to your Google account
3. Request permission to access your Google Drive files
4. Save the authentication token for future use

**Note**: The application only requests read-only access to your Google Drive files.

## Examples

### Indexing Documents
```bash
# Index your documents folder
$ python main.py index ~/Documents
Starting indexing of directory: /Users/username/Documents
Starting to index directory: /Users/username/Documents
Processed 100 files...
Processed 200 files...
Indexing complete. Processed 245 files, skipped 12 files.
Indexed 245 documents.
Index saved to: /Users/username/Documents/index.pkl
```

### Searching Content
```bash
# Search for "python programming"
$ python main.py search "python programming"
Searching for: 'python programming'

--- Search Results (3 found) ---
1. File: /Users/username/Documents/projects/python_tutorial.pdf
   Snippet: ...Python programming language is widely used for web development, data science, and automation...
2. File: /Users/username/Documents/notes/programming_notes.txt
   Snippet: ...Learn Python programming basics including variables, loops, and functions...
3. File: /Users/username/Documents/work/code_review.docx
   Snippet: ...The Python programming team needs to review the new API implementation...
```

### Search with Directory Option
```bash
# Search in a specific directory using its default index
$ python main.py search "javascript" --directory ~/Documents/projects
Searching for: 'javascript'

--- Search Results (2 found) ---
1. File: /Users/username/Documents/projects/web_app.js
   Snippet: ...JavaScript code for the web application...
```

### Semantic Search Examples
```bash
# Build semantic index
$ python main.py semantic-index ~/Documents
Starting semantic indexing of directory: /Users/username/Documents
Using model: all-MiniLM-L6-v2
ChromaDB path: ./chroma_db
Semantic indexing complete!
Files processed: 245
Chunks created: 1200
Files skipped: 12

# Semantic search for concepts
$ python main.py semantic-search "machine learning algorithms"
Performing semantic search for: 'machine learning algorithms'
ChromaDB path: ./chroma_db

--- Semantic Search Results (3 found) ---
1. File: ai_research.pdf
   Path: /Users/username/Documents/research/ai_research.pdf
   Chunk: 2/5
   Similarity: 0.892
   Snippet: The study explores various machine learning algorithms including neural networks, decision trees, and support vector machines...

# Hybrid search combining semantic and keyword matching
$ python main.py semantic-search "data analysis techniques" --hybrid
Performing semantic search for: 'data analysis techniques'
ChromaDB path: ./chroma_db
Using hybrid search (semantic + keyword matching)...

--- Semantic Search Results (2 found) ---
1. File: statistics_guide.docx
   Path: /Users/username/Documents/guides/statistics_guide.docx
   Chunk: 1/3
   Combined Score: 0.945
   Snippet: This guide covers statistical analysis methods, data visualization techniques, and hypothesis testing procedures...
```

### Google Drive Examples

**Setup and index Google Drive:**
```bash
# Setup credentials
$ python main.py setup-gdrive ~/Downloads/credentials.json
Setting up Google Drive credentials from: /Users/username/Downloads/credentials.json
Google Drive credentials set up successfully!
You can now use Google Drive indexing and search commands.

# Index Google Drive files
$ python main.py gdrive-index --folder-id "1ABC123DEF456"
Starting Google Drive indexing (folder_id: 1ABC123DEF456)
Starting to index Google Drive files (folder_id: 1ABC123DEF456)
Processed 50 files...
Google Drive indexing complete. Processed 78 files, skipped 5 files.
Indexed 78 documents.
Index saved to: gdrive_index_1ABC123DEF456.pkl
```

**Search Google Drive directly:**
```bash
# Real-time search in Google Drive
$ python main.py gdrive-search "project proposal"
Searching Google Drive for: 'project proposal'

--- Google Drive Search Results (2 found) ---
1. File: Q1_Project_Proposal.docx
   Type: application/vnd.openxmlformats-officedocument.wordprocessingml.document
   Snippet: This project proposal outlines the development of a new customer management system...
2. File: Budget_Proposal_2024.xlsx
   Type: application/vnd.openxmlformats-officedocument.spreadsheetml.sheet
   Snippet: The budget proposal includes detailed cost breakdowns for the upcoming fiscal year...
```

**Smart hybrid indexing and search:**
```bash
# Index both local and Google Drive files (smart indexing)
$ python main.py hybrid-index ~/Documents --gdrive-folder-id "1ABC123DEF456"
Starting smart hybrid indexing of local directory: /Users/username/Documents
Including Google Drive folder: 1ABC123DEF456
Existing index detected - using incremental indexing
Local changes detected: 2 new, 1 modified, 0 deleted
Google Drive changes detected: 3 new, 0 modified, 1 deleted

--- Hybrid Indexing Results (incremental) ---
Total documents: 325
New files: 5
Modified files: 1
Deleted files: 1
Skipped files: 319
Index saved to: /Users/username/Documents/index.pkl

# Search the hybrid index
$ python main.py search "quarterly report" --directory ~/Documents
Searching for: 'quarterly report'

--- Search Results (4 found) ---
1. File: gdrive://1DEF456GHI789
   Snippet: The quarterly report shows significant growth in Q3 with revenue increasing by 15%...
2. File: /Users/username/Documents/work/Q3_Report.pdf
   Snippet: This quarterly report covers the performance metrics for the third quarter...
```

**Smart hybrid semantic indexing and search:**
```bash
# Build semantic index for both local and Google Drive files (smart indexing)
$ python main.py hybrid-semantic-index ~/Documents --gdrive-folder-id "1ABC123DEF456"
Starting smart hybrid semantic indexing of local directory: /Users/username/Documents
Including Google Drive folder: 1ABC123DEF456
Using model: all-MiniLM-L6-v2
ChromaDB path: ./chroma_db
Existing semantic index detected - using incremental indexing
Local changes detected: 1 new, 0 modified, 0 deleted
Google Drive changes detected: 2 new, 1 modified, 0 deleted

--- Hybrid Semantic Indexing Results (incremental) ---
Total files processed: 325
Total chunks created: 1520
New files: 3
Modified files: 1
Deleted files: 0
Skipped files: 321

# Semantic search across both sources
$ python main.py hybrid-semantic-search "artificial intelligence research"
Performing hybrid semantic search for: 'artificial intelligence research'
ChromaDB path: ./chroma_db
Using semantic search...

--- Hybrid Semantic Search Results (3 found) ---
1. ‚òÅÔ∏è AI_Research_Paper.docx
   Path: gdrive://1DEF456GHI789
   Chunk: 2/5
   Similarity: 0.892
   Snippet: The research explores various artificial intelligence algorithms including neural networks...

2. üìÅ machine_learning_notes.pdf
   Path: /Users/username/Documents/research/machine_learning_notes.pdf
   Chunk: 1/3
   Similarity: 0.845
   Snippet: This document covers artificial intelligence fundamentals and research methodologies...

# Hybrid search (semantic + keyword)
$ python main.py hybrid-semantic-search "data visualization techniques" --hybrid
Performing hybrid semantic search for: 'data visualization techniques'
ChromaDB path: ./chroma_db
Using hybrid search (semantic + keyword matching)...

--- Hybrid Semantic Search Results (2 found) ---
1. üìÅ statistics_guide.docx
   Path: /Users/username/Documents/guides/statistics_guide.docx
   Chunk: 1/3
   Combined Score: 0.945
   Snippet: This guide covers statistical analysis methods, data visualization techniques...
```

### Incremental Indexing

**Traditional indexing vs incremental indexing:**
```bash
# Traditional indexing (rebuilds everything)
$ python main.py index ~/Documents
Starting indexing of directory: /Users/username/Documents
Indexing complete. Indexed 1000 documents.

# Incremental indexing (only new/modified files)
$ python main.py incremental-index ~/Documents
Starting incremental indexing...
Local directory: /Users/username/Documents
Metadata tracking file: ./index_metadata.json
Local changes detected: 5 new, 2 modified, 1 deleted

--- Incremental Indexing Results ---
Total files in index: 1004
New files processed: 5
Modified files processed: 2
Deleted files removed: 1
Files skipped (no changes): 993
```

**Incremental semantic indexing:**
```bash
# Incremental semantic indexing for local files
$ python main.py incremental-semantic-index ~/Documents
Starting incremental semantic indexing...
ChromaDB path: ./chroma_db
Model: all-MiniLM-L6-v2
Metadata tracking file: ./index_metadata.json
Local changes detected: 3 new, 1 modified, 0 deleted

--- Incremental Semantic Indexing Results ---
Total files processed: 1004
Total chunks created: 5000
New files processed: 3
Modified files processed: 1
Deleted files removed: 0
Files skipped (no changes): 1000
```

**Incremental indexing with Google Drive:**
```bash
# Incremental indexing for both local and Google Drive
$ python main.py incremental-index ~/Documents --gdrive-folder-id "1ABC123DEF456"
Starting incremental indexing...
Local directory: /Users/username/Documents
Google Drive folder ID: 1ABC123DEF456
Metadata tracking file: ./index_metadata.json
Local changes detected: 2 new, 1 modified, 0 deleted
Google Drive changes detected: 3 new, 0 modified, 1 deleted

--- Incremental Indexing Results ---
Total files in index: 1205
New files processed: 5
Modified files processed: 1
Deleted files removed: 1
Files skipped (no changes): 1198
```

**Incremental semantic indexing with Google Drive:**
```bash
# Incremental semantic indexing for both sources
$ python main.py incremental-semantic-index ~/Documents --gdrive-folder-id "1ABC123DEF456"
Starting incremental semantic indexing...
ChromaDB path: ./chroma_db
Model: all-MiniLM-L6-v2
Metadata tracking file: ./index_metadata.json
Local directory: /Users/username/Documents
Google Drive folder ID: 1ABC123DEF456
Local changes detected: 1 new, 0 modified, 0 deleted
Google Drive changes detected: 2 new, 1 modified, 0 deleted

--- Incremental Semantic Indexing Results ---
Total files processed: 1205
Total chunks created: 6000
New files processed: 3
Modified files processed: 1
Deleted files removed: 0
Files skipped (no changes): 1201
```

### Smart Indexing

**Automatic Detection:**
- **Index Detection**: Automatically detects if an index already exists for the given path
- **Smart Choice**: Uses incremental indexing when possible, full indexing when needed
- **Transparent Operation**: No manual intervention required - just works
- **Force Override**: Use `--force-full` flag to always rebuild from scratch

**Detection Methods:**
- **Metadata Files**: Checks for existing `index_metadata.json` files
- **Index Files**: Looks for existing `index.pkl` files in the directory
- **ChromaDB Collections**: Detects existing semantic indexes
- **Directory Tracking**: Checks if directory is already tracked in metadata

**Example Behavior:**
```bash
# First run - full indexing
$ python main.py index ~/Documents
Starting smart indexing of directory: /Users/username/Documents
No existing index found - performing full indexing
Indexing complete (full).
Total files: 1000
Index saved to: /Users/username/Documents/index.pkl

# Second run - incremental indexing
$ python main.py index ~/Documents
Starting smart indexing of directory: /Users/username/Documents
Existing index detected - using incremental indexing
Local changes detected: 5 new, 2 modified, 1 deleted
Indexing complete (incremental).
Total files: 1004
New files: 5
Modified files: 2
Deleted files: 1
Skipped files: 992
```

### How Incremental Indexing Works

**Change Detection:**
- **File Metadata**: Tracks file size, modification time, and SHA256 hash
- **Smart Comparison**: Compares current file state with previously indexed state
- **Change Types**: Detects new files, modified files, and deleted files
- **Metadata Storage**: Stores tracking information in `index_metadata.json`

**Performance Benefits:**
- **Speed**: 10-100x faster than full re-indexing for large collections
- **Efficiency**: Only processes files that have actually changed
- **Resource Usage**: Minimal CPU and memory usage for incremental updates
- **Scalability**: Scales well with large document collections

**Supported Scenarios:**
- **Local Files**: Tracks file system changes using metadata and hashes
- **Google Drive Files**: Tracks changes using Google Drive API metadata
- **Hybrid Collections**: Handles both local and cloud files simultaneously
- **Semantic Indexing**: Works with both traditional and semantic indexes
```

## Supported File Formats

### Local Files
| Format | Extension | Description | Dependencies |
|--------|-----------|-------------|--------------|
| PDF | `.pdf` | Portable Document Format | PyPDF2 |
| Word | `.docx` | Microsoft Word documents | python-docx |
| Excel | `.xlsx` | Microsoft Excel spreadsheets | openpyxl |
| PowerPoint | `.pptx` | Microsoft PowerPoint presentations | python-pptx |
| Text | `.txt` | Plain text files | Built-in |

### Google Drive Files
| Format | MIME Type | Description | Support |
|--------|-----------|-------------|---------|
| Google Docs | `application/vnd.google-apps.document` | Google Docs documents | Full text extraction |
| Google Sheets | `application/vnd.google-apps.spreadsheet` | Google Sheets spreadsheets | CSV export |
| Google Slides | `application/vnd.google-apps.presentation` | Google Slides presentations | Text export |
| PDF | `application/pdf` | PDF files in Google Drive | Full text extraction |
| Word | `application/vnd.openxmlformats-officedocument.wordprocessingml.document` | Word documents | Full text extraction |
| Excel | `application/vnd.openxmlformats-officedocument.spreadsheetml.sheet` | Excel spreadsheets | Full text extraction |
| PowerPoint | `application/vnd.openxmlformats-officedocument.presentationml.presentation` | PowerPoint presentations | Full text extraction |
| Text | `text/plain` | Plain text files | Full text extraction |

## Architecture

The project follows a modular architecture with clear separation of concerns:

- **`main.py`**: Entry point that sets up the Python path and invokes the CLI
- **`cli_commands/cli.py`**: Command-line interface using Click framework
- **`pkg/file_parsers/parsers.py`**: File format parsers for text extraction
- **`pkg/indexer/core.py`**: Indexing engine with inverted index and TF-IDF
- **`pkg/indexer/semantic.py`**: Semantic indexing using ChromaDB and sentence-transformers
- **`pkg/indexer/semantic_hybrid.py`**: Hybrid semantic indexing for local + Google Drive files
- **`pkg/indexer/google_drive.py`**: Google Drive indexing and integration
- **`pkg/indexer/incremental.py`**: Incremental indexing with change detection for local and cloud files
- **`pkg/utils/google_drive.py`**: Google Drive API client and authentication
- **`pkg/searcher/core.py`**: Search engine with ranking and snippet generation

### Key Components

1. **File Parsers**: Extract text from various file formats with error handling
2. **Indexer**: Builds inverted index for fast searching with tokenization and stop word filtering
3. **Semantic Indexer**: Creates document embeddings and stores them in ChromaDB for semantic search
4. **Hybrid Semantic Indexer**: Creates embeddings for both local and Google Drive files in a unified ChromaDB collection
5. **Google Drive Client**: Handles authentication, file listing, and content extraction from Google Drive
6. **Google Drive Indexer**: Indexes Google Drive files and integrates with local indexing
7. **Incremental Indexer**: Tracks file changes and performs efficient incremental updates
8. **Searcher**: Performs ranked search using TF-IDF scoring with snippet generation
9. **Semantic Searcher**: Performs semantic similarity search using sentence embeddings
10. **CLI**: User-friendly command-line interface with help and error handling

## Performance

### **Indexing Performance**
- **Local Files**: Processes ~1000 files per minute (varies by file size and type)
- **Google Drive Files**: Processes ~100-200 files per minute (limited by API rate limits)
- **Semantic Indexing**: Creates embeddings for ~500 chunks per minute
- **Memory Usage**: Efficient memory usage with file size limits (50MB per file)
- **Storage Efficiency**: Index files are typically 10-20% of original document size

### **Search Performance**
- **Keyword Search**: Returns results in milliseconds
- **Semantic Search**: Returns results in 1-3 seconds (includes embedding generation)
- **Hybrid Search**: Combines both approaches for optimal speed and accuracy
- **Offline Search**: No network latency when searching indexed content

### **Scalability**
- **Large Collections**: Handles directories with 10,000+ files
- **Progress Tracking**: Real-time progress updates during indexing
- **Incremental Updates**: Add new files without rebuilding entire indexes
- **Memory Management**: Automatic cleanup and efficient resource usage

### **Resource Usage**
- **Disk Space**: ChromaDB typically uses 2-5x the original text size
- **RAM Usage**: ~100MB base + ~10MB per 1000 indexed documents
- **CPU Usage**: Moderate during indexing, minimal during search
- **Network**: Only required during Google Drive indexing, not during search

## Semantic Search Features

### Document Chunking
- **Smart Chunking**: Documents are split into overlapping chunks (1000 characters by default)
- **Sentence Boundary Awareness**: Chunks are created at sentence boundaries when possible
- **Overlap**: 200-character overlap between chunks to maintain context

### Embedding Models
- **Default Model**: `all-MiniLM-L6-v2` (fast and efficient)
- **Alternative Models**: Support for other sentence transformer models
- **Model Loading**: Automatic model download and caching

### Search Types
- **Semantic Search**: Pure semantic similarity using sentence embeddings
- **Hybrid Search**: Combines semantic similarity with keyword matching
- **Configurable Thresholds**: Adjustable similarity thresholds for result filtering

### Vector Database
- **ChromaDB**: Persistent vector database for storing embeddings
- **Cosine Similarity**: Uses cosine distance for similarity calculations
- **Efficient Retrieval**: Fast similarity search with configurable result limits

## Configuration

### File Size Limits
- Maximum file size: 50MB (configurable in `parsers.py`)
- Large files are automatically skipped to prevent memory issues

### Skip Patterns
The indexer automatically skips:
- Hidden files and directories (starting with `.`)
- System files (`.DS_Store`, `.pyc`, etc.)
- Development directories (`node_modules`, `__pycache__`, etc.)
- Version control directories (`.git`, `.svn`, etc.)
- Temporary files (`.tmp`, `.log`, etc.)
- IDE directories (`.vscode`, `.idea`, etc.)

### Stop Words
Common stop words are filtered out during indexing and search:
- Articles: the, a, an
- Prepositions: in, on, at, to, for, of, with, by
- Conjunctions: and, or, but
- Common verbs: is, are, was, were, have, has, had, do, does, did
- Pronouns: i, you, he, she, it, we, they, me, him, her, us, them

## Testing

The project includes comprehensive test coverage:

### Running Tests
```bash
# Run all tests
python -m pytest tests/

# Run specific test modules
python -m pytest tests/pkg/file_parsers/test_parsers.py
python -m pytest tests/pkg/indexer/test_core.py
python -m pytest tests/pkg/searcher/test_core.py
python -m pytest tests/test_integration.py

# Run tests with coverage
python -m pytest tests/ --cov=pkg --cov=cmd
```

### Test Individual Components
```bash
# Test file parsers
python -m pkg.file_parsers.parsers

# Test indexer
python -m pkg.indexer.core

# Test searcher
python -m pkg.searcher.core
```

### Test Types
- **Unit Tests**: Test individual functions and classes
- **Integration Tests**: Test complete workflows and CLI commands
- **Error Handling Tests**: Test edge cases and error conditions
- **Performance Tests**: Test with larger datasets

## Troubleshooting

### Common Issues

**"Import could not be resolved" errors:**
- Ensure all dependencies are installed: `pip install -r requirements.txt`
- Restart your IDE/editor to refresh the Python environment
- Check that you're using the correct Python interpreter

**Large files not indexed:**
- Files larger than 50MB are automatically skipped
- Check file size and consider splitting large documents
- Verify the file is not corrupted

**No search results:**
- Ensure the directory was indexed successfully
- Check that the search terms are not all stop words
- Verify the index file exists and is not corrupted
- Try using the `--directory` option to specify the correct location

**Memory issues:**
- Reduce the number of files indexed at once
- Increase system memory or use smaller file batches
- Check for memory leaks in large directories

**File parsing errors:**
- Ensure required dependencies are installed for the file format
- Check file permissions and accessibility
- Verify file format is supported

## Development

### Project Structure
```
desktop-search/
‚îú‚îÄ‚îÄ main.py                 # Entry point
‚îú‚îÄ‚îÄ cli_commands/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ cli.py             # Command-line interface
‚îú‚îÄ‚îÄ pkg/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ file_parsers/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ parsers.py     # File format parsers
‚îÇ   ‚îú‚îÄ‚îÄ indexer/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ core.py        # Indexing engine
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ semantic.py    # Semantic indexing
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ semantic_hybrid.py # Hybrid semantic indexing
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ google_drive.py # Google Drive indexing
‚îÇ   ‚îú‚îÄ‚îÄ searcher/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ core.py        # Search engine
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îî‚îÄ‚îÄ google_drive.py # Google Drive API client
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ pkg/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ file_parsers/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_parsers.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ indexer/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_core.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ searcher/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ test_core.py
‚îÇ   ‚îî‚îÄ‚îÄ test_integration.py
‚îú‚îÄ‚îÄ requirements.txt       # Dependencies
‚îî‚îÄ‚îÄ README.md             # This file
```

### Code Quality
- **Type Hints**: Full type annotations throughout the codebase
- **Error Handling**: Comprehensive exception handling and logging
- **Documentation**: Detailed docstrings for all functions and classes
- **Testing**: High test coverage with unit and integration tests

### Adding New Features
1. Create feature branch
2. Add tests for new functionality
3. Implement the feature with proper error handling
4. Update documentation
5. Run all tests to ensure nothing is broken
6. Submit pull request

### Semantic Search Implementation Plan

#### Phase 1: Dependencies and Setup
```bash
# Add to requirements.txt
sentence-transformers>=2.2.0
torch>=1.9.0
```

#### Phase 2: Core Implementation
1. **Create semantic indexer** (`pkg/indexer/semantic.py`):
   - Load sentence transformer model
   - Generate embeddings for document chunks
   - Store embeddings with metadata

2. **Extend searcher** (`pkg/searcher/semantic.py`):
   - Implement semantic similarity search
   - Add hybrid search combining keyword + semantic
   - Configurable similarity thresholds

3. **Update CLI** (`cli_commands/cli.py`):
   - Add `--semantic` flag for semantic search
   - Add `--hybrid` flag for combined search
   - Add `--threshold` option for similarity threshold

#### Phase 3: Advanced Features
- Document chunking for better semantic matching
- Caching for improved performance
- Multiple model support
- Batch processing for large datasets

#### Example Usage (Future)
```bash
# Semantic search
python main.py search "machine learning algorithms" --semantic

# Hybrid search (keyword + semantic)
python main.py search "python programming" --hybrid --threshold 0.7

# Semantic search with custom model
python main.py search "data analysis" --semantic --model all-mpnet-base-v2
```

## Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Make your changes with proper tests
4. Commit your changes (`git commit -m 'Add amazing feature'`)
5. Push to the branch (`git push origin feature/amazing-feature`)
6. Open a Pull Request

### Development Guidelines
- Follow PEP 8 style guidelines
- Add type hints to all functions
- Write tests for new functionality
- Update documentation as needed
- Ensure all tests pass before submitting

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## TODO

### Planned Features

#### Semantic Search with Sentence Transformers
- **Integration**: Add support for semantic search using sentence-transformers
- **Model**: Use `all-MiniLM-L6-v2` for local semantic embeddings
- **Features**:
  - Semantic similarity search (understanding meaning, not just keywords)
  - Hybrid search combining keyword and semantic matching
  - Configurable similarity thresholds
  - Support for multiple embedding models
- **Implementation**:
  - Add `sentence-transformers` dependency
  - Create semantic index alongside keyword index
  - Implement semantic search in searcher module
  - Add CLI options for semantic vs keyword search

#### Additional Enhancements
- **Fuzzy Search**: Add fuzzy matching for typos and variations
- **Advanced Filtering**: Filter by file type, date, size
- **Search History**: Remember and suggest previous searches
- **GUI Interface**: Web-based or desktop GUI
- **Real-time Indexing**: Watch directories for changes
- **Cloud Storage**: Support for Google Drive, Dropbox, etc.

## Acknowledgments

- Built with [Click](https://click.palletsprojects.com/) for the command-line interface
- Uses [PyPDF2](https://pypdf2.readthedocs.io/) for PDF processing
- Inspired by modern search engine architectures
- Thanks to the Python community for excellent libraries and tools
